{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:02.127845Z","iopub.execute_input":"2025-04-08T11:30:02.128421Z","iopub.status.idle":"2025-04-08T11:30:02.133979Z","shell.execute_reply.started":"2025-04-08T11:30:02.128376Z","shell.execute_reply":"2025-04-08T11:30:02.132616Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class SelfAttention(nnx.Module):\n    def __init__(self, num_heads, d_model, rngs: nnx.Rngs):\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.q_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n        self.k_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n        self.v_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n        self.out_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n\n    def __call__(self, x):\n        batch, seq_len, d_model = x.shape\n        head_dim = d_model // self.num_heads\n\n        # Compute queries, keys, and values\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        # Reshape to (batch, seq_len, num_heads, head_dim) then transpose to (batch, num_heads, seq_len, head_dim)\n        q = q.reshape(batch, seq_len, self.num_heads, head_dim)\n        k = k.reshape(batch, seq_len, self.num_heads, head_dim)\n        v = v.reshape(batch, seq_len, self.num_heads, head_dim)\n        q = jnp.transpose(q, (0, 2, 1, 3))\n        k = jnp.transpose(k, (0, 2, 1, 3))\n        v = jnp.transpose(v, (0, 2, 1, 3))\n\n        # Scaled dot-product attention\n        scale = head_dim ** -0.5\n        attn_logits = jnp.einsum('bhqd,bhkd->bhqk', q, k) * scale\n        attn_weights = jax.nn.softmax(attn_logits, axis=-1)\n        attn_output = jnp.einsum('bhqk,bhvd->bhqd', attn_weights, v)\n\n        # Transpose back and reshape to (batch, seq_len, d_model)\n        attn_output = jnp.transpose(attn_output, (0, 2, 1, 3))\n        attn_output = attn_output.reshape(batch, seq_len, d_model)\n        output = self.out_proj(attn_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:03.391505Z","iopub.execute_input":"2025-04-08T11:30:03.391972Z","iopub.status.idle":"2025-04-08T11:30:03.402787Z","shell.execute_reply.started":"2025-04-08T11:30:03.391935Z","shell.execute_reply":"2025-04-08T11:30:03.401428Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MLPBlock(nnx.Module):\n    def __init__(self, d_model, d_hidden, rngs: nnx.Rngs):\n        self.linear1 = nnx.Linear(d_model, d_hidden, rngs=rngs)\n        self.linear2 = nnx.Linear(d_hidden, d_model, rngs=rngs)\n\n    def __call__(self, x):\n        x = nnx.gelu(self.linear1(x))\n        x = self.linear2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:07.450995Z","iopub.execute_input":"2025-04-08T11:30:07.451458Z","iopub.status.idle":"2025-04-08T11:30:07.457942Z","shell.execute_reply.started":"2025-04-08T11:30:07.451425Z","shell.execute_reply":"2025-04-08T11:30:07.456634Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class TransformerBlock(nnx.Module):\n    def __init__(self, d_model, num_heads, d_hidden, rngs: nnx.Rngs):\n        # Use BatchNorm for simplicity (alternatively, LayerNorm can be used)\n        self.ln1 = nnx.BatchNorm(d_model, rngs=rngs)\n        self.attn = SelfAttention(num_heads, d_model, rngs=rngs)\n        self.ln2 = nnx.BatchNorm(d_model, rngs=rngs)\n        self.mlp = MLPBlock(d_model, d_hidden, rngs=rngs)\n\n    def __call__(self, x):\n        attn_out = self.attn(self.ln1(x))\n        x = x + attn_out\n        mlp_out = self.mlp(self.ln2(x))\n        x = x + mlp_out\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:11.999730Z","iopub.execute_input":"2025-04-08T11:30:12.000100Z","iopub.status.idle":"2025-04-08T11:30:12.006820Z","shell.execute_reply.started":"2025-04-08T11:30:12.000071Z","shell.execute_reply":"2025-04-08T11:30:12.005563Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class MiniGPT(nnx.Module):\n    def __init__(self, vocab_size, seq_len, d_model, num_heads, d_hidden, num_layers, rngs: nnx.Rngs):\n        self.vocab_size = vocab_size\n        self.seq_len = seq_len\n        self.token_embed = nnx.Embed(vocab_size, d_model, rngs=rngs)\n        # Initialize positional embedding as a parameter (learned)\n        self.pos_embed = nnx.Param(jnp.zeros((seq_len, d_model)), sharding=None)\n        # Create a list of Transformer blocks\n        self.blocks = [TransformerBlock(d_model, num_heads, d_hidden, rngs=rngs) for _ in range(num_layers)]\n        self.ln_f = nnx.BatchNorm(d_model, rngs=rngs)\n        self.head = nnx.Linear(d_model, vocab_size, rngs=rngs)\n\n    def __call__(self, x):\n        # x shape: (batch, seq_len) containing token ids\n        x = self.token_embed(x)  # (batch, seq_len, d_model)\n        x = x + self.pos_embed    # add positional embedding\n        for block in self.blocks:\n            x = block(x)\n        x = self.ln_f(x)\n        logits = self.head(x)     # (batch, seq_len, vocab_size)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:15.208657Z","iopub.execute_input":"2025-04-08T11:30:15.209047Z","iopub.status.idle":"2025-04-08T11:30:15.216942Z","shell.execute_reply.started":"2025-04-08T11:30:15.209011Z","shell.execute_reply":"2025-04-08T11:30:15.215738Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"@nnx.jit\ndef loss_fn(model, x, y, smoothing=0.1):\n    logits = model(x)  # shape: (batch, seq_len, vocab_size)\n    # Convert integer labels to one-hot vectors\n    one_hot = jax.nn.one_hot(y, num_classes=logits.shape[-1])\n    # Apply label smoothing: for correct class use (1 - smoothing), distribute smoothing uniformly to all classes.\n    smoothed_labels = one_hot * (1 - smoothing) + smoothing / logits.shape[-1]\n    # Compute log softmax of logits\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    # Compute the mean cross-entropy loss\n    loss = -jnp.mean(jnp.sum(smoothed_labels * log_probs, axis=-1))\n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:17.294616Z","iopub.execute_input":"2025-04-08T11:30:17.295039Z","iopub.status.idle":"2025-04-08T11:30:17.302419Z","shell.execute_reply.started":"2025-04-08T11:30:17.295002Z","shell.execute_reply":"2025-04-08T11:30:17.301003Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"@nnx.jit\ndef train_step(model, optimizer, x, y):\n    loss, grads = nnx.value_and_grad(loss_fn)(model, x, y)\n    optimizer.update(grads)  # In-place update of model parameters via shared references\n    return loss\n\n@nnx.jit\ndef eval_step(model, x, y):\n    logits = model(x)\n    loss = loss_fn(model, x, y)\n    predictions = jnp.argmax(logits, axis=-1)\n    accuracy = jnp.mean(predictions == y)\n    return loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:19.238190Z","iopub.execute_input":"2025-04-08T11:30:19.238656Z","iopub.status.idle":"2025-04-08T11:30:19.246410Z","shell.execute_reply.started":"2025-04-08T11:30:19.238620Z","shell.execute_reply":"2025-04-08T11:30:19.244770Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"vocab_size = 100\nseq_len = 16\nbatch_size = 32\nnum_batches = 100  # Number of training batches for this demo\n\n# Initialize a random key for synthetic data generation.\nrng = jax.random.PRNGKey(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:20.566597Z","iopub.execute_input":"2025-04-08T11:30:20.567012Z","iopub.status.idle":"2025-04-08T11:30:20.573499Z","shell.execute_reply.started":"2025-04-08T11:30:20.566978Z","shell.execute_reply":"2025-04-08T11:30:20.572207Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def get_batch(rng):\n    rng, subkey = jax.random.split(rng)\n    x = jax.random.randint(subkey, (batch_size, seq_len), 0, vocab_size)\n    rng, subkey = jax.random.split(rng)\n    y = jax.random.randint(subkey, (batch_size, seq_len), 0, vocab_size)\n    return rng, x, y\n\nd_model = 32      # Embedding and hidden dimension\nnum_heads = 4     # Number of attention heads\nd_hidden = 64     # Hidden dimension for the MLP in each Transformer block\nnum_layers = 2    # Number of Transformer blocks\n\n# Create a PRNG stream for Flax NNX initialization.\nrngs = nnx.Rngs(0)\n\n# Instantiate the model\nmodel = MiniGPT(vocab_size, seq_len, d_model, num_heads, d_hidden, num_layers, rngs=rngs)\n\n# Create the optimizer; the optimizer shares a reference with the model for in-place updates.\noptimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-3))\n\nfor i in range(num_batches):\n    rng, x_batch, y_batch = get_batch(rng)\n    loss = train_step(model, optimizer, x_batch, y_batch)\n    if i % 10 == 0:\n        print(f\"Batch {i:03d}, Loss: {loss:.4f}\")\n\nnum_eval_batches = 20\neval_losses = []\neval_accuracies = []\n\nfor i in range(num_eval_batches):\n    rng, x_eval, y_eval = get_batch(rng)\n    loss, acc = eval_step(model, x_eval, y_eval)\n    eval_losses.append(loss)\n    eval_accuracies.append(acc)\n\navg_eval_loss = jnp.mean(jnp.array(eval_losses))\navg_eval_accuracy = jnp.mean(jnp.array(eval_accuracies))\nprint(f\"Evaluation Loss: {avg_eval_loss:.4f}, Evaluation Accuracy: {avg_eval_accuracy*100:.2f}%\")\n\nrng, x_infer, _ = get_batch(rng)\nlogits = model(x_infer)\nprint(\"Inference logits shape:\", logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:30:22.628788Z","iopub.execute_input":"2025-04-08T11:30:22.629203Z","iopub.status.idle":"2025-04-08T11:30:28.621178Z","shell.execute_reply.started":"2025-04-08T11:30:22.629170Z","shell.execute_reply":"2025-04-08T11:30:28.619937Z"}},"outputs":[{"name":"stdout","text":"Batch 000, Loss: 5.0854\nBatch 010, Loss: 4.9538\nBatch 020, Loss: 4.8708\nBatch 030, Loss: 4.7964\nBatch 040, Loss: 4.7713\nBatch 050, Loss: 4.6911\nBatch 060, Loss: 4.6878\nBatch 070, Loss: 4.6713\nBatch 080, Loss: 4.6481\nBatch 090, Loss: 4.6484\nEvaluation Loss: 4.6365, Evaluation Accuracy: 0.90%\nInference logits shape: (32, 16, 100)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}